{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils.data_preprocess import load_data\n",
    "from utils.early_stop import EarlyStopping\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from utils.comm_evaluate import CommunityEval\n",
    "import numpy as np\n",
    "from utils.common_utils import random_aug\n",
    "from OCDIB_model_v1 import OCDIB\n",
    "from utils.common_utils import label_to_comm, aff_to_ovlp_label, print_result, weight_incrementer, filter_by_types\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import OCDIB_config as config\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import BCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.1,\n",
       " 'checkpoint_path': '/home/lml/OCDIB/OCDIB/checkpoint/Amazon',\n",
       " 'patience': 50,\n",
       " 'test_size': 0.2,\n",
       " 'label_rate': 0.8,\n",
       " 'edge_drop_rate': 0.1,\n",
       " 'mod_weight': 1000.0,\n",
       " 'ib_weight': 1,\n",
       " 'kl_weight': 1e-07,\n",
       " 'contras_weight': 1,\n",
       " 'tempreture': 5}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.train_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5231 1308\n"
     ]
    }
   ],
   "source": [
    "# data_config = {'dataset':'LFR','network':'5000-500-2'}\n",
    "DataLoader = load_data(data_config=config.data_config)\n",
    "G = DataLoader.G\n",
    "train_id, test_id, train_y, test_y = DataLoader.get_train_test_data(test_rate=config.train_config['test_size'])\n",
    "train_id = train_id[: int(config.train_config['label_rate']*(len(train_id)+len(test_id)))]\n",
    "train_y = train_y[: int(config.train_config['label_rate']*(len(train_id)+len(test_id)))]\n",
    "print(len(train_id), len(test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6539"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.num_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = DataLoader.label_dict\n",
    "l = [len(v) for k,v in label_dict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for comparison between overlapping and non-overlapping nodes\n",
    "# m_ids, o_ids, no_ids = filter_by_types(G, DataLoader.label_dict)\n",
    "# print(len(m_ids), len(o_ids), len(no_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "row = []\n",
    "col = []\n",
    "data = []\n",
    "for idx, com_list in DataLoader.label_dict.items():\n",
    "    row.extend([idx] * len(com_list))\n",
    "    col.extend(com_list)\n",
    "    # data.extend([1 / len(com_list)] * len(com_list)) # normalized\n",
    "    data.extend([1] * len(com_list))\n",
    "matrix = sp.csr_matrix((data, (row, col)), shape=(G.number_of_nodes(), DataLoader.K), dtype=np.float64)\n",
    "aff_ground_truth = torch.Tensor(matrix.toarray()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OCDIB(input_dim=G.num_nodes(), \n",
    "            hidden_channels_list=config.model_config['hidden_channels_list'], \n",
    "            output_dim=config.model_config['output_dim'], \n",
    "            K=DataLoader.K, device=device)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.train_config['lr'])\n",
    "ck_path = os.path.join('./checkpoint', config.data_config['dataset']+config.data_config['network'])\n",
    "stopper = EarlyStopping(checkpoint_path=ck_path, patience=config.train_config['patience'], is_ours=True)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.027619815312126515"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = G.num_edges() / G.num_nodes() / (G.num_nodes()-1)\n",
    "threshold = np.sqrt(-(np.log2(1-eps)))\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train]: Epoch: 001 | loss:0.7421 | ib_weight: | ib_loss:0.74 | mod_loss:0.06 | contras_loss: | contras_loss1: | kl1:\n",
      "[Train]: Epoch: 002 | loss:0.6863 | ib_weight: | ib_loss:0.69 | mod_loss:0.06 | contras_loss: | contras_loss1: | kl1:\n",
      "[Train]: Epoch: 003 | loss:0.6232 | ib_weight: | ib_loss:0.62 | mod_loss:0.08 | contras_loss: | contras_loss1: | kl1:\n",
      "[Train]: Epoch: 004 | loss:0.5569 | ib_weight: | ib_loss:0.56 | mod_loss:0.05 | contras_loss: | contras_loss1: | kl1:\n",
      "[Train]: Epoch: 005 | loss:0.4167 | ib_weight: | ib_loss:0.42 | mod_loss:0.06 | contras_loss: | contras_loss1: | kl1:\n",
      "[Train]: Epoch: 006 | loss:0.2990 | ib_weight: | ib_loss:0.30 | mod_loss:0.06 | contras_loss: | contras_loss1: | kl1:\n",
      "[Train]: Epoch: 007 | loss:0.2357 | ib_weight: | ib_loss:0.24 | mod_loss:0.05 | contras_loss: | contras_loss1: | kl1:\n",
      "[Train]: Epoch: 008 | loss:0.2089 | ib_weight: | ib_loss:0.21 | mod_loss:0.04 | contras_loss: | contras_loss1: | kl1:\n",
      "[Train]: Epoch: 009 | loss:0.1919 | ib_weight: | ib_loss:0.19 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "[Train]: Epoch: 010 | loss:0.1614 | ib_weight: | ib_loss:0.16 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "[Train]: Epoch: 011 | loss:0.1397 | ib_weight: | ib_loss:0.14 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "[Train]: Epoch: 012 | loss:0.1304 | ib_weight: | ib_loss:0.13 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "[Train]: Epoch: 013 | loss:0.1202 | ib_weight: | ib_loss:0.12 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "[Train]: Epoch: 014 | loss:0.1111 | ib_weight: | ib_loss:0.11 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "[Train]: Epoch: 015 | loss:0.1078 | ib_weight: | ib_loss:0.11 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "[Train]: Epoch: 016 | loss:0.0961 | ib_weight: | ib_loss:0.10 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "[Train]: Epoch: 017 | loss:0.0901 | ib_weight: | ib_loss:0.09 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "[Train]: Epoch: 018 | loss:0.0788 | ib_weight: | ib_loss:0.08 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "[Train]: Epoch: 019 | loss:0.0825 | ib_weight: | ib_loss:0.08 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 1 out of 50\n",
      "[Train]: Epoch: 020 | loss:0.0881 | ib_weight: | ib_loss:0.09 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 2 out of 50\n",
      "[Train]: Epoch: 021 | loss:0.0799 | ib_weight: | ib_loss:0.08 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 3 out of 50\n",
      "[Train]: Epoch: 022 | loss:0.3220 | ib_weight: | ib_loss:0.32 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 4 out of 50\n",
      "[Train]: Epoch: 023 | loss:0.0965 | ib_weight: | ib_loss:0.10 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 5 out of 50\n",
      "[Train]: Epoch: 024 | loss:0.1029 | ib_weight: | ib_loss:0.10 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 6 out of 50\n",
      "[Train]: Epoch: 025 | loss:0.0861 | ib_weight: | ib_loss:0.09 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 7 out of 50\n",
      "[Train]: Epoch: 026 | loss:0.0889 | ib_weight: | ib_loss:0.09 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 8 out of 50\n",
      "[Train]: Epoch: 027 | loss:0.0700 | ib_weight: | ib_loss:0.07 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "[Train]: Epoch: 028 | loss:0.0709 | ib_weight: | ib_loss:0.07 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 1 out of 50\n",
      "[Train]: Epoch: 029 | loss:0.0580 | ib_weight: | ib_loss:0.06 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "[Train]: Epoch: 030 | loss:0.0551 | ib_weight: | ib_loss:0.05 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "[Train]: Epoch: 031 | loss:0.0634 | ib_weight: | ib_loss:0.06 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 1 out of 50\n",
      "[Train]: Epoch: 032 | loss:0.0771 | ib_weight: | ib_loss:0.08 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 2 out of 50\n",
      "[Train]: Epoch: 033 | loss:0.0581 | ib_weight: | ib_loss:0.06 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 3 out of 50\n",
      "[Train]: Epoch: 034 | loss:0.0533 | ib_weight: | ib_loss:0.05 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "[Train]: Epoch: 035 | loss:0.0759 | ib_weight: | ib_loss:0.08 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 1 out of 50\n",
      "[Train]: Epoch: 036 | loss:0.5998 | ib_weight: | ib_loss:0.60 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 2 out of 50\n",
      "[Train]: Epoch: 037 | loss:0.0618 | ib_weight: | ib_loss:0.06 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 3 out of 50\n",
      "[Train]: Epoch: 038 | loss:0.1300 | ib_weight: | ib_loss:0.13 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 4 out of 50\n",
      "[Train]: Epoch: 039 | loss:0.6157 | ib_weight: | ib_loss:0.62 | mod_loss:0.06 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 5 out of 50\n",
      "[Train]: Epoch: 040 | loss:0.9955 | ib_weight: | ib_loss:1.00 | mod_loss:0.05 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 6 out of 50\n",
      "[Train]: Epoch: 041 | loss:0.5144 | ib_weight: | ib_loss:0.51 | mod_loss:0.04 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 7 out of 50\n",
      "[Train]: Epoch: 042 | loss:0.7830 | ib_weight: | ib_loss:0.78 | mod_loss:0.06 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 8 out of 50\n",
      "[Train]: Epoch: 043 | loss:0.9796 | ib_weight: | ib_loss:0.98 | mod_loss:0.04 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 9 out of 50\n",
      "[Train]: Epoch: 044 | loss:0.4083 | ib_weight: | ib_loss:0.41 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 10 out of 50\n",
      "[Train]: Epoch: 045 | loss:0.1374 | ib_weight: | ib_loss:0.14 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 11 out of 50\n",
      "Epoch 00045: reducing learning rate of group 0 to 1.0000e-02.\n",
      "[Train]: Epoch: 046 | loss:0.5062 | ib_weight: | ib_loss:0.51 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 12 out of 50\n",
      "[Train]: Epoch: 047 | loss:0.1416 | ib_weight: | ib_loss:0.14 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 13 out of 50\n",
      "[Train]: Epoch: 048 | loss:0.1392 | ib_weight: | ib_loss:0.14 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 14 out of 50\n",
      "[Train]: Epoch: 049 | loss:0.2600 | ib_weight: | ib_loss:0.26 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 15 out of 50\n",
      "[Train]: Epoch: 050 | loss:0.1348 | ib_weight: | ib_loss:0.13 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 16 out of 50\n",
      "[Train]: Epoch: 051 | loss:0.1190 | ib_weight: | ib_loss:0.12 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 17 out of 50\n",
      "[Train]: Epoch: 052 | loss:0.1156 | ib_weight: | ib_loss:0.12 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 18 out of 50\n",
      "[Train]: Epoch: 053 | loss:0.1022 | ib_weight: | ib_loss:0.10 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 19 out of 50\n",
      "[Train]: Epoch: 054 | loss:0.0896 | ib_weight: | ib_loss:0.09 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 20 out of 50\n",
      "[Train]: Epoch: 055 | loss:15.4242 | ib_weight: | ib_loss:15.42 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 21 out of 50\n",
      "[Train]: Epoch: 056 | loss:0.1214 | ib_weight: | ib_loss:0.12 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 22 out of 50\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.0000e-03.\n",
      "[Train]: Epoch: 057 | loss:0.1432 | ib_weight: | ib_loss:0.14 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 23 out of 50\n",
      "[Train]: Epoch: 058 | loss:0.1176 | ib_weight: | ib_loss:0.12 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 24 out of 50\n",
      "[Train]: Epoch: 059 | loss:4.5583 | ib_weight: | ib_loss:4.56 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 25 out of 50\n",
      "[Train]: Epoch: 060 | loss:0.0808 | ib_weight: | ib_loss:0.08 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 26 out of 50\n",
      "[Train]: Epoch: 061 | loss:0.0758 | ib_weight: | ib_loss:0.08 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 27 out of 50\n",
      "[Train]: Epoch: 062 | loss:0.0776 | ib_weight: | ib_loss:0.08 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 28 out of 50\n",
      "[Train]: Epoch: 063 | loss:0.0826 | ib_weight: | ib_loss:0.08 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 29 out of 50\n",
      "[Train]: Epoch: 064 | loss:2.4369 | ib_weight: | ib_loss:2.44 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 30 out of 50\n",
      "[Train]: Epoch: 065 | loss:0.3595 | ib_weight: | ib_loss:0.36 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 31 out of 50\n",
      "[Train]: Epoch: 066 | loss:86.8271 | ib_weight: | ib_loss:86.83 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 32 out of 50\n",
      "[Train]: Epoch: 067 | loss:0.0760 | ib_weight: | ib_loss:0.08 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 33 out of 50\n",
      "Epoch 00067: reducing learning rate of group 0 to 1.0000e-04.\n",
      "[Train]: Epoch: 068 | loss:0.3025 | ib_weight: | ib_loss:0.30 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 34 out of 50\n",
      "[Train]: Epoch: 069 | loss:0.0828 | ib_weight: | ib_loss:0.08 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 35 out of 50\n",
      "[Train]: Epoch: 070 | loss:0.0740 | ib_weight: | ib_loss:0.07 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 36 out of 50\n",
      "[Train]: Epoch: 071 | loss:0.0968 | ib_weight: | ib_loss:0.10 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 37 out of 50\n",
      "[Train]: Epoch: 072 | loss:0.0801 | ib_weight: | ib_loss:0.08 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 38 out of 50\n",
      "[Train]: Epoch: 073 | loss:0.0925 | ib_weight: | ib_loss:0.09 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 39 out of 50\n",
      "[Train]: Epoch: 074 | loss:0.0778 | ib_weight: | ib_loss:0.08 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 40 out of 50\n",
      "[Train]: Epoch: 075 | loss:39.6898 | ib_weight: | ib_loss:39.69 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 41 out of 50\n",
      "[Train]: Epoch: 076 | loss:0.0796 | ib_weight: | ib_loss:0.08 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 42 out of 50\n",
      "[Train]: Epoch: 077 | loss:0.0779 | ib_weight: | ib_loss:0.08 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 43 out of 50\n",
      "[Train]: Epoch: 078 | loss:0.0740 | ib_weight: | ib_loss:0.07 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 44 out of 50\n",
      "Epoch 00078: reducing learning rate of group 0 to 1.0000e-05.\n",
      "[Train]: Epoch: 079 | loss:0.0752 | ib_weight: | ib_loss:0.07 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 45 out of 50\n",
      "[Train]: Epoch: 080 | loss:0.0977 | ib_weight: | ib_loss:0.10 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 46 out of 50\n",
      "[Train]: Epoch: 081 | loss:59.4506 | ib_weight: | ib_loss:59.45 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 47 out of 50\n",
      "[Train]: Epoch: 082 | loss:0.4917 | ib_weight: | ib_loss:0.49 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 48 out of 50\n",
      "[Train]: Epoch: 083 | loss:59.6397 | ib_weight: | ib_loss:59.64 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 49 out of 50\n",
      "[Train]: Epoch: 084 | loss:0.0866 | ib_weight: | ib_loss:0.09 | mod_loss:0.03 | contras_loss: | contras_loss1: | kl1:\n",
      "EarlyStopping counter: 50 out of 50\n",
      "[eval]: 0.84764\t0.32948\t0.74995\t0.27274\t0.85447\t0.86665\t-0.03141\t \n"
     ]
    }
   ],
   "source": [
    "repeat_times = config.evaluate_config['repeat_times']\n",
    "ib_weight = 0.0001\n",
    "for epoch in range(1, 5000+1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    G1 = random_aug(G, edge_drop_rate=config.train_config['edge_drop_rate'])\n",
    "\n",
    "    edge_index_1 = G1.edges()\n",
    "    edge_index_1 = torch.LongTensor(np.array([i.cpu().detach().numpy() for i in edge_index_1])).to(device)\n",
    "    feature = torch.eye(G.num_nodes()).to(device)\n",
    "\n",
    "    hgcn_1, mu_1, logvar_1, hk_1, r_1 = model(x=feature, edge_index=edge_index_1)\n",
    "    mod_loss = model.loss_mod(G, r_1)\n",
    "    cl1 = BCELoss()(r_1[train_id], aff_ground_truth[train_id])\n",
    "    kl1 = -0.5*(1+2*logvar_1[train_id]-mu_1[train_id].pow(2)-logvar_1[train_id].exp()).sum(1).mean().div(math.log(2))\n",
    "    ib_loss = cl1 + config.train_config['kl_weight']*kl1\n",
    "    loss = ib_loss + 0.01*mod_loss\n",
    "    loss.backward()\n",
    "\n",
    "    print('[Train]: Epoch: {:03d} | loss:{:.4f} | ib_weight: | ib_loss:{:.2f} | mod_loss:{:.2f} | contras_loss: | contras_loss1: | kl1:'\n",
    "        .format(epoch, loss, ib_loss, mod_loss))\n",
    "\n",
    "    # Early stopping\n",
    "    early_stop = stopper.step(loss=loss.detach().cpu().numpy(), model=model)\n",
    "\n",
    "        \n",
    "    # Evaluate\n",
    "    if epoch % config.evaluate_config['eval_interval'] == 0 or early_stop:\n",
    "        model.eval()\n",
    "        edge_index = torch.LongTensor(np.array([i.cpu().detach().numpy() for i in G.edges()])).to(device)\n",
    "        for j in range(repeat_times):\n",
    "            hgcn, mu, logvar, hk, r = model(feature, edge_index)\n",
    "            pred_y = aff_to_ovlp_label(r.detach().cpu().numpy(), threshold=threshold)\n",
    "            Evaluator = CommunityEval(train_id, test_id, test_y, pred_y, DataLoader.K, G)\n",
    "            score_dict = Evaluator.eval_community(is_overlapping=DataLoader.is_overlapping, affiliation_matrix=r.detach().cpu())\n",
    "            if j == 0:\n",
    "                final_score_dict = score_dict\n",
    "            else:\n",
    "                # update the scores in the final dict\n",
    "                final_score_dict = {k: final_score_dict[k]+v for k,v in score_dict.items()}\n",
    "        final_score_dict = {k: v/repeat_times for k,v in final_score_dict.items()}\n",
    "        print_result(final_score_dict)\n",
    "\n",
    "    # Optimize\n",
    "    optimizer.step()\n",
    "    scheduler.step(loss)\n",
    "\n",
    "    if early_stop:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for comparison between overlapping and non-overlapping nodes\n",
    "# old_test_id = test_id\n",
    "# old_test_y = test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[eval]: 0.84784\t0.31235\t0.74811\t0.33976\t0.86177\t0.85027\t-0.02737\t \n"
     ]
    }
   ],
   "source": [
    "model = stopper.load_checkpoint(model)\n",
    "# model = stopper.load_checkpoint(model, filepath='early_stop_2023-11-09_22-49')\n",
    "repeat_times = config.evaluate_config['repeat_times']\n",
    "edge_index = G.edges()\n",
    "edge_index = torch.LongTensor(np.array([i.cpu().detach().numpy() for i in edge_index])).to(device)\n",
    "# Evaluate\n",
    "\n",
    "# used for comparison between overlapping and non-overlapping nodes\n",
    "# test_id = []\n",
    "# test_y = []\n",
    "# for i, id in enumerate(old_test_id):\n",
    "#     if id in no_ids:\n",
    "#         test_id.append(id)\n",
    "#         test_y.append(old_test_y[i])\n",
    "# print(len(old_test_id), len(test_id))\n",
    "\n",
    "\n",
    "model.eval()\n",
    "feature = torch.eye(G.num_nodes()).to(device)\n",
    "for j in range(repeat_times):\n",
    "    hgcn, mu, logvar, hk, r = model(feature, edge_index)\n",
    "    pred_y = aff_to_ovlp_label(r.detach().cpu().numpy(), threshold=threshold)\n",
    "    Evaluator = CommunityEval(train_id, test_id, test_y, pred_y, DataLoader.K, G)\n",
    "    score_dict = Evaluator.eval_community(is_overlapping=DataLoader.is_overlapping, affiliation_matrix=r.detach().cpu())\n",
    "    if j == 0:\n",
    "        final_score_dict = score_dict\n",
    "    else:\n",
    "        # update the scores in the final dict\n",
    "        final_score_dict = {k: final_score_dict[k]+v for k,v in score_dict.items()}\n",
    "final_score_dict = {k: v/repeat_times for k,v in final_score_dict.items()}\n",
    "print_result(final_score_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c78076ae78e0ac626653ebe38eceba1dc14c030b7210645d3804c87f549a68bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
